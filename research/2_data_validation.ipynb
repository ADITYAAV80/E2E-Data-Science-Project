{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aditya/MLOPS/E2E-Data-Science-Project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"artifacts/data_ingestion/winequality-red.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           float64\n",
       "volatile acidity        float64\n",
       "citric acid             float64\n",
       "residual sugar          float64\n",
       "chlorides               float64\n",
       "free sulfur dioxide     float64\n",
       "total sulfur dioxide    float64\n",
       "density                 float64\n",
       "pH                      float64\n",
       "sulphates               float64\n",
       "alcohol                 float64\n",
       "quality                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class DataValidationConfig(BaseModel):\n",
    "\n",
    "    root_dir: Path\n",
    "    STATUS_FILE: str\n",
    "    unzip_data_dir: Path\n",
    "    all_schema: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DataScienceWorkflow.constants import *\n",
    "from src.DataScienceWorkflow.utils.common import read_yaml,create_directories\n",
    "from box import Box\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH,\n",
    "                 schema_filepath = SCHEMA_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_validation_config(self)->DataValidationConfig:\n",
    "\n",
    "        config = self.config.data_validation\n",
    "        schema = self.schema.COLUMNS\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            STATUS_FILE = config.STATUS_FILE,\n",
    "            unzip_data_dir = config.unzip_data_dir,\n",
    "            all_schema = schema\n",
    "        )\n",
    "\n",
    "        return data_validation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-29 16:26:30,759: INFO: common : Yaml File: config/config.yaml loaded successfully]\n",
      "[2025-03-29 16:26:30,760: INFO: common : Yaml File: params.yaml loaded successfully]\n",
      "[2025-03-29 16:26:30,761: INFO: common : Yaml File: schema.yaml loaded successfully]\n",
      "[2025-03-29 16:26:30,761: INFO: common : created directory at: artifacts]\n",
      "[2025-03-29 16:26:30,762: INFO: common : created directory at: artifacts/data_validation]\n"
     ]
    }
   ],
   "source": [
    "cm = ConfigurationManager()\n",
    "data_validation_config = cm.get_data_validation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DataScienceWorkflow import logger\n",
    "\n",
    "class DataValiadtion:\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def validate_all_columns(self) -> bool:\n",
    "        try:\n",
    "            validation_status = True  # Assume valid unless proven wrong\n",
    "            \n",
    "            ## Read CSV\n",
    "            data = pd.read_csv(self.config.unzip_data_dir)\n",
    "\n",
    "            ## Extract all columns and schema\n",
    "            all_cols = list(data.columns)\n",
    "            all_schema = set(self.config.all_schema.keys())  # Convert to set for fast lookup\n",
    "\n",
    "            ## Check for missing or extra columns\n",
    "            missing_cols = all_schema - set(all_cols)\n",
    "            extra_cols = set(all_cols) - all_schema\n",
    "\n",
    "            if missing_cols or extra_cols:\n",
    "                validation_status = False\n",
    "\n",
    "            ## Write final status\n",
    "            with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                f.write(f\"Validation status: {validation_status}\\n\")\n",
    "                if missing_cols:\n",
    "                    f.write(f\"Missing Columns: {missing_cols}\\n\")\n",
    "                if extra_cols:\n",
    "                    f.write(f\"Extra Columns: {extra_cols}\\n\")\n",
    "\n",
    "            return validation_status\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv = DataValiadtion(data_validation_config)\n",
    "dv.validate_all_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E2E-Data-Sciene-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
